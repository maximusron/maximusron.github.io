---
layout: post
title: "Google Summer of Code 2023 @CERN-HSF"
author: "Aaron"
tags: GSoc Google Summer of Code CERN-HSF
---

This is a blog post entailing the cumulative work I achieved during my 4 months as a GSoC student under the CERN-HSF organization.

Here is a link to the project proposal : 

- [**Extending the Cppyy support in Numba**](https://docs.google.com/document/d/1Cmi27sKKkvSX4bNBMvyz56VK2Oq10RD2AikOKuyiySk/edit?usp=drive_link)

Presentations at the Compiler Research Group: 

- [**May 17, 2023**](https://compiler-research.org/assets/presentations/CaaS_Weekly_17_05_2023_Aaron-Extending_the_Cppyy_Support_in_Numba.pdf)

- [**Aug 16, 2023**](https://compiler-research.org/assets/presentations/CaaS_Weekly_16_08_2023_Aaron-Extending_the_Cppyy_Support_in_Numba_Progress_Update.pdf)

# Experience

Participating in Google Summer of Code has been incredibly rewarding for me. I had the opportunity to immerse myself in the advanced computing environment at CERN and interact with technologies in the ROOT ecosystem like Cling. My project, "Extending the Cppyy support in Numba," allowed me to delve deep into the architecture of Cppyy, enabling me to understand its structure and significance. This experience has enriched my knowledge base and elevated my problem-solving and coding abilities, helping me see the broader implications of software development tools and their applications in real-world scenarios. Working on a mixed C++ and Python codebase on a daily basis subconsciously elevated my problem-solving skills in other aspects of my life, like university or other coding challenges

My experience was quite fantastic, and I was glad that I could succesfully contribute to the amazing work in high performance and interactive computing being done at CERN.  My favourite part of Google Summer of Code was the exposure to software development on the highest level, specifically in compiler research. I believe CERN-HSF has given me a platform to systematically explore and work in high-performance computing. This also includes the wonderful conversations I had with my mentors and other group members that definitely contributed to improving my knowledge and ideas. I am very grateful for the footing I have obtained in performing open-source contributions in a streamlined fashion.

# Extending the Cppyy support in Numba

![alt-text-1](FinalSubmission0.jpg "title-1") ![alt-text-2](FinalSubmission1.jpg "title-2")

|  |   |
| :-- | --: |
| __Cppyy__ : An automatic, run-time, Python\-C\+\+ bindings generator <br> __Cling__ : is used in backend since an interactive C\+\+ interpreter provides a runtime exec approach to C\+\+ code <br>__Numba__ : A JIT compiler that translates Python and NumPy code into fast machine code| ![](cppyy_arch.png) |

## Why use Numba?

The compute time overhead while switching between languages accumulates in loops with cppyy objects.

Numba optimizes the loop and compiles it into machine code which crosses the language barrier only once

![alt-text-1](FinalSubmission4.jpg "title-1") ![alt-text-2](FinalSubmission5.jpg "title-2")

## Challenges

Typing is one of the largest problems posed: Template function utilization, reference types and correct function matching depend on the type resolution system

Type Inference solution: A mechanism to handle implicit casting based on propagated type info and the cppyy reflection layer.

Note: Typing does not backtrack since the numba extension will only ever obtain the numba type inference result.

| Python | Numba Type | LLVM Type used in Numba lowering |
| :-: | :-: | :-: |
| 3 (int) | int64 | i64 |
| 3.14 (float) | float64 | double |
| (1, 2, 3) | UniTuple(int64, 3) | [3 x i64] |
| (1, 2.5) | Tuple(int64, float64) | {i64, double} |
| np.array([1, 2], dtype=np.int32) | array(int64, 1d, C) | {i8*, i8*, i64, i64, i32*, [1 x i64]} |
| “Hello” | unicode_type | {i8*, i64, i32, i32, i64, i8*, i8*} |

---

This is an important step to addressing the challenges on the avenues of adding efficient reference type detection and handling, as well as extending the capabilities of the current template functions supported.

## Primary Deliverables:

- Add general support for C\+\+ templates in Numba through Cppyy

- Add support for C\+\+ reference types in Numba through Cppyy

## Numba pipeline

![](numba_cmp.png)

__Typing__ : Numba core has a type inference algorithm which assigns a nb_type for a variable

__Lowering__ : Numba lowers high-level Python operations into low-level LLVM code.Exploits typing to map to LLVM type

__Boxing and unboxing__ : convert `PyObject*`‘s into native values, and vice-versa.

We utilise the runtime numba compilation process to lower C\+\+ code cppdef’ed in Python. How?

## Numba Low Level Extension API in Cppyy:

![Alt text](numba_ext.png)

# Final project Status

Currently, the following functionality has been added to Cppyy’s Numba extension:

- Extended typing and non type template definition support [Test 9]
- nJIT function pointers to C++ functions that return a reference type [Test 10]
- nJIT support for pointers and reference types to builtins and std::vectors [Test 11- 13]
- nJIT(non-boxing calls) for Eigen templated classes passed-by-ref [Test 14]

### Pull Requests during GSoC:

[https://github\.com/wlav/cppyy/pull/177](https://github.com/wlav/cppyy/pull/177)

[https://github\.com/wlav/cppyy\-backend/pull/11](https://github.com/wlav/cppyy-backend/pull/11)

[https://github\.com/wlav/cppyy\-backend/pull/14](https://github.com/wlav/cppyy-backend/pull/14)

[https://github\.com/compiler\-research/xeus\-clang\-repl/pull/56](https://github.com/compiler-research/xeus-clang-repl/pull/56)

[https://github\.com/compiler\-research/xeus\-clang\-repl/pull/57](https://github.com/compiler-research/xeus-clang-repl/pull/57)

[https://github\.com/compiler\-research/xeus\-clang\-repl/pull/58](https://github.com/compiler-research/xeus-clang-repl/pull/58)

[https://github\.com/compiler\-research/xeus\-clang\-repl/pull/60](https://github.com/compiler-research/xeus-clang-repl/pull/60)

[https://github\.com/compiler\-research/xeus\-clang\-repl/pull/61](https://github.com/compiler-research/xeus-clang-repl/pull/61)

[https://github\.com/compiler\-research/xeus\-clang\-repl/pull/62](https://github.com/compiler-research/xeus-clang-repl/pull/62)

[https://github\.com/compiler\-research/xeus\-clang\-repl/pull/63](https://github.com/compiler-research/xeus-clang-repl/pull/63)

## Pointer and Reference Type Support

The Numba extension now supports njitting ref types, const refs and pointers to C\+\+ methods/functions.

The results are reflected directly on the python side using the ctypes interface that provides a “pointer\-like” behaviour that can be emulated in Python

![](ptr_ref.png)

The “pointer” like behavior is especially useful in cases like these

![](ptr_ref2.png)

The fact that Numba lowers Cppyy calls that use C\+\+ pointers, to LLVM IR, opens the avenue of significant speedups

![](FinalSubmission18.png)

## `STD::VECTOR<>*` and Numpy Arrays

We can explore those speedups by also adding pointer and reference support to std::vector objects

This is achieved by constructing IR Pointer Types to Array and Vector Types\, that point to `cppyy.gbl.std.vector()` objects linked to numpy arrays for initialization


![alt-text-1](FinalSubmission19.jpg "title-1") ![alt-text-2](FinalSubmission20.jpg "title-2")

Here the members of the BoxVector class are initialized via pass\-by\-ref within the python function call



![](benchmarks1.png)

### Initial benchmarks with NumPy\-C\+\+ equivalent functions for the same operations:

| Experiment<br />[Time in milliseconds] | NumPy <br />(Standard Python loop) | NumPy NJIT <br />(Equivalent LLVM IR ) | Cppyy NJIT<br /> |
| :-: | :-: | :-: | :-: |
| 1 | 0.19 | 246.39 | <span style="color: green;">0.042</span> |
| 2 | 0.22 | 291.74 | <span style="color: green;">0.058</span> |
| 3 | 77.95 | 368.68 | <span style="color: green;">17.94</span> |
| 4 | 92.36 | 374.97 | <span style="color: green;">20.38</span> |



### Exploring vectorization speedups with a dot product operation:

Initialising using pointers and running the dot product through pass\-by\-ref
We can achieve an even faster dot product by using the DotVector datamembers which are in turn `std::vector` pointers:

![](FinalSubmission28.png)

Usage : 

![alt-text-1](FinalSubmission30.png "title-1") ![alt-text-2](FinalSubmission29.jpg "title-2")


# Some Benchmark Trends

![](FinalSubmission31.png)

![](FinalSubmission32.png)

## Added Eigen Support

Templated class args like `Eigen::Matrix<Scalar, Rows, Cols, Options, MaxRows, MaxCols>` are resolved to C\+\+ types and successfully matches the CPPOverload. The Numba typeinfer of the Eigen metaclass is refactored into the C\+\+ expression and handled in `numba2cpp`. This support currenbtly works only for `Eigen::Dense`.

Dispatcher typeinfer :

`CppClass(Eigen::Matrix<double,-1,-1,0,-1,-1>)`

![](FinalSubmission33.png)

![](FinalSubmission34.png)

![](FinalSubmission35.png)

![](FinalSubmission36.png)

## Exciting Results with Eigen:

### - The overhead of crossing the language barrier is eliminated:

![](FinalSubmission38.png)

### - Comparison with NumPy `np.dot` for the same matrix dimensions:

![](FinalSubmission39.png)



